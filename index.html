<!DOCTYPE html>
<html>
<head>
    <meta charset="utf-8">
    <title>Editing Implicit Assumptions in Text-to-Image Diffusion Models</title>
    <link href="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha384-1BmE4kWBq78iYhFldvKuhfTAU6auU8tT94WrHftjDbrCEXSU1oBoqyl2QvZ6jIW3" crossorigin="anonymous">
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
    <link href="fontawesome-5.15.4/css/all.css" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css?family=Google+Sans" rel="stylesheet">
    <link href="styles.css" rel="stylesheet" />
    <script type="text/javascript" src="scripts.js"></script>
</head>
<body style="font-family: 'Google Sans', sans-serif;" onload="window.addEventListener('scroll', reveal); reveal();">
    <div class="container" style="margin-top:40px;">
        <div class="row">
            <div class="col" style="text-align:center;">
                <h1 style="font-size:2.3rem;font-weight:bold;">Editing Implicit Assumptions in Text-to-Image Diffusion Models</h1>
                <center><table style="width:85%;margin-bottom:12px;">
                <tr><td class="pad-cell"></td>
                <td class="author-cell"><a href="https://orgadhadas.github.io/">Hadas Orgad</a><sup>*</sup></td>
                <td class="author-cell"><a href="https://bahjat-kawar.github.io/">Bahjat Kawar</a><sup>*</sup></td>
                <td class="author-cell"><a href="https://www.cs.technion.ac.il/~belinkov/">Yonatan Belinkov</a></td>
                <td class="pad-cell"></td></tr>
                <tr>
                <td class="eq-cell" colspan="2"><small><sup>*</sup>Equal contribution.</small></td>
                <td class="institution-cell">Technion</td>
                <td colspan="2"></td></tr></table></center>
                <a href="https://arxiv.org/abs/2303.08084" class="btn btn-success"><i class="ai ai-arxiv"></i> ArXiv</a>
                <a href="TIME_paper.pdf" type="button" class="btn btn-danger"><i class="far fa-file-pdf"></i> PDF</a>
                <a href="https://github.com/bahjat-kawar/time-diffusion" type="button" class="btn btn-dark"><i class="fab fa-github"></i> Code</a>
                <a href="https://huggingface.co/spaces/bahjat-kawar/time-diffusion" type="button" class="btn btn-warning btn-gradio">&#129303; Demo</a><br />
                <br />
                <img src="images/figure1.jpg" class="headline-image"><br />
                We edit a model based on a source and destination prompt. The edit generalizes to related prompts (green), leaving unrelated ones unaffected (gray).<br /><br />
            </div>
        </div>
        <div class="row">
            <div class="col abstract-col">
                <h2>Abstract</h2>
                Text-to-image diffusion models often make implicit assumptions about the world when generating images.
                While some assumptions are useful (e.g., the sky is blue), they can also be <span class="marked">outdated, incorrect, or reflective of social biases</span> present in the training data.
                Thus, there is a need to <span class="marked">control these assumptions without requiring explicit user input</span> or costly re-training.
                In this work, we aim to edit a given implicit assumption in a pre-trained diffusion model.
                Our Text-to-Image Model Editing method, TIME for short, receives a pair of inputs: a "source" under-specified prompt for which the model makes an implicit assumption (e.g., "a pack of roses"), and a "destination" prompt that describes the same setting, but with a specified desired attribute (e.g., "a pack of blue roses").
                TIME then updates the model's cross-attention layers, as these layers assign visual meaning to textual tokens.
                We edit the projection matrices in these layers such that the source prompt is projected close to the destination prompt.
                Our method is highly efficient, as it <span class="marked">modifies a mere 2.2% of the model's parameters in under one second.</span>
                To evaluate model editing approaches, we introduce TIMED (TIME Dataset), containing 147 source and destination prompt pairs from various domains.
                Our experiments (using Stable Diffusion) show that TIME is successful in model editing, <span class="marked">generalizes well for related prompts unseen during editing, and imposes minimal effect on unrelated generations.</span>
                <br /><br />
            </div>
        </div>
        <div class="row">
            <div class="col">
                <center><h2>Cross-Attention Layers in Text-to-Image Models</h2></center>
                <img src="images/cross-attention.jpg" style="width:300px;float:left;" />
                The user-provided text prompt is input into the text encoder, which tokenizes it and outputs a sequence of token embeddings \(\{\mathbf{c}_{i}\}_{i=1}^{l}\) describing the sentenceâ€™s meaning.
                Then, in order to condition the diffusion model on them, these embeddings are injected at the cross-attention layers of the model. They are projected into keys \(\mathbf{K}\) and values \(\mathbf{V}\), using learned projection matrices \(\mathbf{W}_{K}\) and \(\mathbf{W}_{V}\), respectively. The keys are then multiplied by a query \(\mathbf{Q}\), which represents visual features of the current intermediate image in the diffusion process.
                This results in an attention map, which is then multiplied by the values \(\mathbf{V}\) to produce the final cross-attention output.
                We focus our editing efforts on \(\mathbf{W}_{K}\) and \(\mathbf{W}_{V}\), as they map the textual information into visual concepts.
                <br /><br />
            </div>
        </div>
        <div class="row">
            <div class="col">
                <center><h2>TIME: Text-to-Image Model Editing</h2></center>
                <img src="images/time-method.jpg" style="width:300px;float:right;" />
                <p>Our method accepts a pair of source and destination text prompts.
                For each source embedding \(\mathbf{c}_{i}\), we identify the destination embedding that corresponds to the same token, and denote it as \(\mathbf{c}^{*}_{i}\).
                The keys and values of the destination prompts are calculated as \(\mathbf{k}^{*}_{i} = \mathbf{W}_{K} \mathbf{c}^{*}_{i}\) and \(\mathbf{v}^{*}_{i} = \mathbf{W}_{V} \mathbf{c}^{*}_{i}\).
                We then optimize for new projection matrices \(\mathbf{W}'_{K}\) and \(\mathbf{W}'_{V}\) that minimize the loss function:</p>
                <p>\( \sum_{i=1}^{l}
                \left\lVert {\mathbf{W}'}_K \mathbf{c}_i - \mathbf{k}^{*}_{i} \right\rVert_2^2
                + \lambda \left\lVert {\mathbf{W}'}_K - {\mathbf{W}}_K \right\rVert_F^2
                + \sum_{i=1}^{l}
                \left\lVert {\mathbf{W}'}_V \mathbf{c}_i - \mathbf{v}^{*}_{i} \right\rVert_2^2
                + \lambda \left\lVert {\mathbf{W}'}_V - {\mathbf{W}}_V \right\rVert_F^2 \). </p>
                <p>We prove that this loss function has a closed-form global minimum solution, and we use it to edit text-to-image models efficiently (editing Stable Diffusion in around 0.4 seconds).</p>
                <br /><br />
            </div>
        </div>
        <div class="row">
            <div class="col">
                <center><h2>Gender Bias Mitigation</h2></center>
                <center>
                    <img src="images/gender-bias.jpg" style="width:70%;" /><br />
                    We apply TIME for gender bias mitigation. For instance, we erase the assumption that most developers are male.
                </center>
                <br /><br />
            </div>
        </div>
        <div class="row">
            <div class="col">
                <center><h2>Additional Results</h2></center>
                <center>
                    <img src="images/extra-results.jpg" style="width:80%;" /><br />
                    Please refer to <a href="TIME_paper.pdf">our paper</a> for more results, quantitative evaluation, gender bias mitigation experiments, and more.<br />
                    Please refer to <a href="https://github.com/bahjat-kawar/time-diffusion">our GitHub</a> for implementation using Stable Diffusion and the provided datasets.
                </center>
                <br /><br />
            </div>
        </div>
        <div class="row">
            <div class="col-lg-12">
                <center><h2>BibTeX</h2></center>
                <pre><code>@article{orgad2023editing,
    title={Editing Implicit Assumptions in Text-to-Image Diffusion Models},
    author={Orgad, Hadas and Kawar, Bahjat and Belinkov, Yonatan},
    journal={arXiv:2303.08084},
    year={2023}
}</code></pre>
            </div>
        </div>
    </div>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@5.1.3/dist/js/bootstrap.bundle.min.js" integrity="sha384-ka7Sk0Gln4gmtz2MlQnikT1wXgYsOg+OMhuP+IlRH9sENBO0LRn5q+8nbTov4+1p" crossorigin="anonymous"></script>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
<script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
</body>
</html>